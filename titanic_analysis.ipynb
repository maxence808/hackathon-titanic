{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Load Data\n",
    "\n",
    "First, let's import the necessary libraries and load our training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test: (891, 12) (418, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theod\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:18: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1) IMPORTS & LOAD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5  # mets 3 pour itérer vite, 5 pour le run final\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "train_df = pd.read_csv(DATA_DIR/\"train.csv\")\n",
    "test_df  = pd.read_csv(DATA_DIR/\"test.csv\")\n",
    "print(\"Train/Test:\", train_df.shape, test_df.shape)\n",
    "assert train_df.shape[0]==891 and test_df.shape[0]==418, \"Fichiers Titanic inattendus.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Now, let's explore the data to understand its structure, find patterns, and identify missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) HELPERS: features avancées + target encoding anti-fuite\n",
    "\n",
    "def extract_title(name: pd.Series) -> pd.Series:\n",
    "    t = name.str.extract(r',\\s*([^.]+)\\.', expand=False).fillna('Unknown')\n",
    "    map_title = {'Mlle':'Miss','Ms':'Miss','Mme':'Mrs','Lady':'Rare','Countess':'Rare','Sir':'Rare',\n",
    "                 'Jonkheer':'Rare','Don':'Rare','Dona':'Rare','Capt':'Rare','Col':'Rare','Dr':'Rare',\n",
    "                 'Major':'Rare','Rev':'Rare'}\n",
    "    t = t.replace(map_title)\n",
    "    return t.where(t.isin(['Mr','Mrs','Miss','Master']), 'Rare')\n",
    "\n",
    "def split_ticket(t: str):\n",
    "    t = str(t).strip()\n",
    "    if \" \" in t:\n",
    "        prefix, number = t.rsplit(\" \", 1)\n",
    "    else:\n",
    "        prefix, number = \"NONE\", t\n",
    "    return prefix, number\n",
    "\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Groupes\n",
    "    out['Surname']    = out['Name'].str.split(',').str[0].str.strip()\n",
    "    out['FamilySize'] = out['SibSp'].fillna(0) + out['Parch'].fillna(0) + 1\n",
    "    out['FamilyID']   = (out['Surname'] + \"_\" + out['FamilySize'].astype(int).astype(str)).astype(str)\n",
    "\n",
    "    # Titre\n",
    "    out['Title'] = extract_title(out['Name'])\n",
    "\n",
    "    # Ticket prefix/number & group size\n",
    "    tp, tn = zip(*out['Ticket'].apply(split_ticket))\n",
    "    out['Ticket_prefix'] = list(tp)\n",
    "    out['Ticket_number'] = list(tn)\n",
    "    out['Ticket_clean']  = out['Ticket'].astype(str).str.replace(r'[^A-Za-z0-9]', '', regex=True)\n",
    "    counts = out['Ticket_clean'].value_counts()\n",
    "    out['TicketGroupSize'] = out['Ticket_clean'].map(counts).fillna(1).clip(upper=6).astype(int)\n",
    "\n",
    "    # Cabines\n",
    "    out['HasCabin']  = (~out['Cabin'].isna()).astype(int)\n",
    "    out['CabinDeck'] = out['Cabin'].astype(str).str[0]\n",
    "    out['CabinDeck'] = out['CabinDeck'].where(out['CabinDeck'].isin(list('ABCDEFGT')), 'U')\n",
    "\n",
    "    # Embarked\n",
    "    if out['Embarked'].isna().any():\n",
    "        out['Embarked'] = out['Embarked'].fillna(out['Embarked'].mode().iloc[0])\n",
    "\n",
    "    # Fare & ratios\n",
    "    out['Fare'] = out['Fare'].replace(0, np.nan)\n",
    "    out['Fare'] = out['Fare'].fillna(out.groupby('Pclass')['Fare'].transform('median'))\n",
    "    out['IsAlone'] = (out['FamilySize']==1).astype(int)\n",
    "    out['FarePerPerson'] = (out['Fare'] / out['FamilySize']).replace([np.inf,-np.inf], np.nan)\n",
    "    out['FarePerPerson'] = out['FarePerPerson'].fillna(out['Fare'].median())\n",
    "\n",
    "    # Age imput (par Title/Pclass/Sex)\n",
    "    out['Age'] = out['Age'].fillna(out.groupby(['Title','Pclass','Sex'])['Age'].transform('median'))\n",
    "\n",
    "    # Bins\n",
    "    out['AgeBin']  = pd.qcut(out['Age'].clip(0,80), 6, duplicates='drop').cat.codes\n",
    "    out['FareBin'] = pd.qcut(out['Fare'].clip(0,250), 6, duplicates='drop').cat.codes\n",
    "\n",
    "    # Interactions\n",
    "    out['Sex_is_male'] = (out['Sex']=='male').astype(int)\n",
    "    out['IsChild']     = (out['Age'] < 14).astype(int)\n",
    "    out['SexxPclass']  = out['Sex_is_male'] * out['Pclass']\n",
    "    out['SexxAge']     = out['Sex_is_male'] * out['Age']\n",
    "    out['FarePPxP']    = out['FarePerPerson'] * out['Pclass']\n",
    "\n",
    "    return out\n",
    "\n",
    "def kfold_target_encode(train, test, col, target='Survived', n_splits=5, smoothing=20, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = train[target].mean()\n",
    "    oof = pd.Series(index=train.index, dtype=float)\n",
    "\n",
    "    for tr_idx, va_idx in skf.split(train, train[target]):\n",
    "        tr, va = train.iloc[tr_idx], train.iloc[va_idx]\n",
    "        stats = tr.groupby(col)[target].agg(['mean','count'])\n",
    "        stats['enc'] = (stats['mean']*stats['count'] + global_mean*smoothing) / (stats['count'] + smoothing)\n",
    "        oof.iloc[va_idx] = va[col].map(stats['enc']).fillna(global_mean)\n",
    "\n",
    "    stats_full = train.groupby(col)[target].agg(['mean','count'])\n",
    "    stats_full['enc'] = (stats_full['mean']*stats_full['count'] + global_mean*smoothing) / (stats['count'] + smoothing)\n",
    "    test_enc = test[col].map(stats_full['enc']).fillna(global_mean)\n",
    "\n",
    "    return oof.values.astype(float), test_enc.values.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Feature Engineering\n",
    "\n",
    "Based on our EDA, we'll clean the data by handling missing values and create new features to improve our model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (891, 43)  | X_test: (418, 43)  | y: (891,)\n",
      "NA -> 0 / 0\n"
     ]
    }
   ],
   "source": [
    "# 3) BUILD MATRICES ++ (features métier + TE K-Fold étendu + one-hot propre)\n",
    "\n",
    "# --- features enrichies\n",
    "def make_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # Groupes\n",
    "    out['Surname']    = out['Name'].str.split(',').str[0].str.strip()\n",
    "    out['FamilySize'] = out['SibSp'].fillna(0) + out['Parch'].fillna(0) + 1\n",
    "    out['FamilyID']   = (out['Surname'] + \"_\" + out['FamilySize'].astype(int).astype(str)).astype(str)\n",
    "\n",
    "    # Titre\n",
    "    def extract_title(name: pd.Series) -> pd.Series:\n",
    "        t = name.str.extract(r',\\s*([^.]+)\\.', expand=False).fillna('Unknown')\n",
    "        map_title = {'Mlle':'Miss','Ms':'Miss','Mme':'Mrs','Lady':'Rare','Countess':'Rare','Sir':'Rare',\n",
    "                     'Jonkheer':'Rare','Don':'Rare','Dona':'Rare','Capt':'Rare','Col':'Rare','Dr':'Rare',\n",
    "                     'Major':'Rare','Rev':'Rare'}\n",
    "        t = t.replace(map_title)\n",
    "        return t.where(t.isin(['Mr','Mrs','Miss','Master']), 'Rare')\n",
    "    out['Title'] = extract_title(out['Name'])\n",
    "\n",
    "    # Ticket prefix/number & clean & group size\n",
    "    def split_ticket(t: str):\n",
    "        t = str(t).strip()\n",
    "        if \" \" in t:\n",
    "            prefix, number = t.rsplit(\" \", 1)\n",
    "        else:\n",
    "            prefix, number = \"NONE\", t\n",
    "        return prefix, number\n",
    "    tp, tn = zip(*out['Ticket'].apply(split_ticket))\n",
    "    out['Ticket_prefix'] = list(tp)\n",
    "    out['Ticket_number'] = list(tn)\n",
    "    out['Ticket_clean']  = out['Ticket'].astype(str).str.replace(r'[^A-Za-z0-9]', '', regex=True)\n",
    "    counts = out['Ticket_clean'].value_counts()\n",
    "    out['TicketGroupSize'] = out['Ticket_clean'].map(counts).fillna(1).clip(upper=6).astype(int)\n",
    "\n",
    "    # Cabines\n",
    "    out['HasCabin']  = (~out['Cabin'].isna()).astype(int)\n",
    "    out['CabinDeck'] = out['Cabin'].astype(str).str[0]\n",
    "    out['CabinDeck'] = out['CabinDeck'].where(out['CabinDeck'].isin(list('ABCDEFGT')), 'U')\n",
    "    # combien de cabines listées (utile pour 1ère classe)\n",
    "    out['CabinCount'] = out['Cabin'].astype(str).str.split().apply(lambda x: 0 if (len(x)==1 and x[0]=='nan') else len(x))\n",
    "\n",
    "    # Embarked\n",
    "    if out['Embarked'].isna().any():\n",
    "        out['Embarked'] = out['Embarked'].fillna(out['Embarked'].mode().iloc[0])\n",
    "\n",
    "    # Fare & ratios\n",
    "    out['Fare'] = out['Fare'].replace(0, np.nan)\n",
    "    out['Fare'] = out['Fare'].fillna(out.groupby('Pclass')['Fare'].transform('median'))\n",
    "    out['IsAlone'] = (out['FamilySize']==1).astype(int)\n",
    "    out['FarePerPerson'] = (out['Fare'] / out['FamilySize']).replace([np.inf,-np.inf], np.nan)\n",
    "    out['FarePerPerson'] = out['FarePerPerson'].fillna(out['Fare'].median())\n",
    "\n",
    "    # Age imput (par Title/Pclass/Sex)\n",
    "    out['Age'] = out['Age'].fillna(out.groupby(['Title','Pclass','Sex'])['Age'].transform('median'))\n",
    "\n",
    "    # Bins\n",
    "    out['AgeBin']  = pd.qcut(out['Age'].clip(0,80), 6, duplicates='drop').cat.codes\n",
    "    out['FareBin'] = pd.qcut(out['Fare'].clip(0,250), 6, duplicates='drop').cat.codes\n",
    "\n",
    "    # Interactions fortes + features métier\n",
    "    out['Sex_is_male'] = (out['Sex']=='male').astype(int)\n",
    "    out['IsChild']     = (out['Age'] < 14).astype(int)\n",
    "    out['IsMother']    = ((out['Sex']=='female') & (out['Parch']>0) & (out['Age']>=18) & (out['Title']!='Miss')).astype(int)\n",
    "    out['SexxPclass']  = out['Sex_is_male'] * out['Pclass']\n",
    "    out['SexxAge']     = out['Sex_is_male'] * out['Age']\n",
    "    out['FarePPxP']    = out['FarePerPerson'] * out['Pclass']\n",
    "\n",
    "    # Longueurs utiles (souvent corrélées)\n",
    "    out['NameLen']     = out['Name'].astype(str).str.len()\n",
    "    out['NameWords']   = out['Name'].astype(str).str.split().str.len()\n",
    "    out['TicketNumLen']= out['Ticket_number'].astype(str).str.len()\n",
    "\n",
    "    # GroupKey pour TE (famille/classe/port)\n",
    "    out['GroupKey'] = out['Surname'].astype(str) + \"_\" + out['Pclass'].astype(str) + \"_\" + out['Embarked'].astype(str)\n",
    "\n",
    "    return out\n",
    "\n",
    "train_f = make_features(train_df)\n",
    "test_f  = make_features(test_df)\n",
    "\n",
    "# --- Target encoding (anti-fuite) sur cardinalités utiles\n",
    "def kfold_target_encode(train, test, col, target='Survived', n_splits=5, smoothing=20, seed=42):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    global_mean = train[target].mean()\n",
    "    oof = pd.Series(index=train.index, dtype=float)\n",
    "    for tr_idx, va_idx in skf.split(train, train[target]):\n",
    "        tr, va = train.iloc[tr_idx], train.iloc[va_idx]\n",
    "        stats = tr.groupby(col)[target].agg(['mean','count'])\n",
    "        stats['enc'] = (stats['mean']*stats['count'] + global_mean*smoothing) / (stats['count'] + smoothing)\n",
    "        oof.iloc[va_idx] = va[col].map(stats['enc']).fillna(global_mean)\n",
    "    stats_full = train.groupby(col)[target].agg(['mean','count'])\n",
    "    stats_full['enc'] = (stats_full['mean']*stats_full['count'] + global_mean*smoothing) / (stats_full['count'] + smoothing)\n",
    "    test_enc = test[col].map(stats_full['enc']).fillna(global_mean)\n",
    "    return oof.values.astype(float), test_enc.values.astype(float)\n",
    "\n",
    "te_cols = ['FamilyID','Surname','Ticket_clean','GroupKey','Ticket_prefix']  # élargi\n",
    "for c in te_cols:\n",
    "    tr_val, te_val = kfold_target_encode(train_f, test_f, c, target='Survived',\n",
    "                                         n_splits=N_SPLITS, smoothing=20, seed=RANDOM_STATE)\n",
    "    train_f[f'TE_{c}'] = tr_val\n",
    "    test_f[f'TE_{c}']  = te_val\n",
    "\n",
    "# --- Drop colonnes brutes texte/IDs\n",
    "drop_cols = [\n",
    "    'Name','Ticket','Cabin','Ticket_clean','Surname','FamilyID',\n",
    "    'Ticket_prefix','Ticket_number','GroupKey'\n",
    "]\n",
    "base_cols_train = [c for c in train_f.columns if c not in drop_cols]\n",
    "base_cols_test  = [c for c in test_f.columns  if c not in drop_cols]\n",
    "common_cols = sorted(set(base_cols_train).intersection(base_cols_test))\n",
    "\n",
    "# --- One-hot (inclure CabinDeck)\n",
    "categoricals = [c for c in ['Sex','Embarked','Title','Pclass','CabinDeck'] if c in common_cols]\n",
    "\n",
    "train_part = train_f[common_cols + ['Survived']].assign(_is_train=1)\n",
    "test_part  = test_f[common_cols].assign(_is_train=0)\n",
    "\n",
    "full = pd.concat([train_part, test_part], ignore_index=True)\n",
    "X_full = pd.get_dummies(full, columns=categoricals, drop_first=True)\n",
    "\n",
    "# --- Split matrices finales\n",
    "train_m = X_full[X_full['_is_train']==1].drop(columns=['_is_train'])\n",
    "test_m  = X_full[X_full['_is_train']==0].drop(columns=['_is_train','Survived'])\n",
    "\n",
    "y        = train_m['Survived'].astype(int).values\n",
    "X_df     = train_m.drop(columns=['Survived','PassengerId']).astype(float)\n",
    "X_testdf = test_m.drop(columns=['PassengerId']).astype(float)\n",
    "pid_test = test_m['PassengerId'].astype(int).values\n",
    "\n",
    "print(\"X:\", X_df.shape, \" | X_test:\", X_testdf.shape, \" | y:\", y.shape)\n",
    "print(\"NA ->\", int(X_df.isna().sum().sum()), \"/\", int(X_testdf.isna().sum().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "It's time to choose a model, train it on our processed data, and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hgb_deep  OOF acc@0.5: 0.8272\n",
      " hgb_reg  OOF acc@0.5: 0.8238\n",
      "      et  OOF acc@0.5: 0.8373\n",
      "      gb  OOF acc@0.5: 0.8451\n",
      "      lr  OOF acc@0.5: 0.8373\n",
      "\n",
      "BEST OOF acc: 0.8586 | th: 0.565 | weights: {'hgb_deep': np.float64(0.122), 'hgb_reg': np.float64(0.073), 'et': np.float64(0.366), 'gb': np.float64(0.366), 'lr': np.float64(0.073)}\n"
     ]
    }
   ],
   "source": [
    "# 4) OOF + ENSEMBLE (HGB deep + HGB régularisé + ExtraTrees + GB + LogReg) + GRID POIDS + SEUIL\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "models = {\n",
    "    'hgb_deep': HistGradientBoostingClassifier(\n",
    "        learning_rate=0.055, max_iter=1200, max_depth=None,\n",
    "        l2_regularization=0.0, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'hgb_reg': HistGradientBoostingClassifier(\n",
    "        learning_rate=0.045, max_iter=1600, max_depth=4,\n",
    "        l2_regularization=0.0, random_state=RANDOM_STATE+1\n",
    "    ),\n",
    "    'et': ExtraTreesClassifier(\n",
    "        n_estimators=1200, max_features=0.5, min_samples_leaf=2,\n",
    "        random_state=RANDOM_STATE+2, n_jobs=-1\n",
    "    ),\n",
    "    'gb': GradientBoostingClassifier(\n",
    "        n_estimators=500, learning_rate=0.06, max_depth=3,\n",
    "        subsample=0.9, random_state=RANDOM_STATE+3\n",
    "    ),\n",
    "    'lr': Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=False)),\n",
    "        ('clf', LogisticRegression(max_iter=4000))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "def oof_proba(model, X, y, cv):\n",
    "    return cross_val_predict(model, X, y, cv=cv, method='predict_proba')[:,1]\n",
    "\n",
    "oof = {}\n",
    "for name, mdl in models.items():\n",
    "    p = oof_proba(mdl, X_df, y, cv)\n",
    "    acc = accuracy_score(y, (p>=0.5).astype(int))\n",
    "    oof[name] = p\n",
    "    print(f\"{name:>8s}  OOF acc@0.5: {acc:.4f}\")\n",
    "\n",
    "# Blending: grille un peu plus large + seuil\n",
    "names = list(oof.keys())\n",
    "P = np.column_stack([oof[n] for n in names])\n",
    "\n",
    "best = {'acc':-1, 'w':None, 'th':0.5}\n",
    "weight_grid = [0.15, 0.25, 0.35, 0.5, 0.75]\n",
    "from itertools import product\n",
    "for w in product(weight_grid, repeat=P.shape[1]):\n",
    "    w = np.array(w, float)\n",
    "    if w.sum()==0: \n",
    "        continue\n",
    "    w = w / w.sum()\n",
    "    blend = (P * w).sum(axis=1)\n",
    "    for th in np.linspace(0.35, 0.65, 61):\n",
    "        acc = accuracy_score(y, (blend >= th).astype(int))\n",
    "        if acc > best['acc']:\n",
    "            best = {'acc':acc, 'w':w.copy(), 'th':float(th)}\n",
    "\n",
    "print(\"\\nBEST OOF acc:\", round(best['acc'],4),\n",
    "      \"| th:\", round(best['th'],3),\n",
    "      \"| weights:\", {n:round(w,3) for n,w in zip(names, best['w'])})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Submission File\n",
    "\n",
    "Finally, we'll use our trained model to make predictions on the test set and generate the submission file in the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote submission.csv — shape (418, 2) | th: 0.565 | weights: {'hgb_deep': np.float64(0.122), 'hgb_reg': np.float64(0.073), 'et': np.float64(0.366), 'gb': np.float64(0.366), 'lr': np.float64(0.073)}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived\n",
       "0          892         0\n",
       "1          893         0\n",
       "2          894         0\n",
       "3          895         0\n",
       "4          896         1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) FIT FINAL + SUBMISSION CSV\n",
    "\n",
    "from sklearn.base import clone\n",
    "import pandas as pd\n",
    "\n",
    "# Fit final de chaque modèle sur tout X_df, y\n",
    "fitted = {}\n",
    "for n, m in models.items():\n",
    "    mdl = clone(m).fit(X_df, y)\n",
    "    fitted[n] = mdl\n",
    "\n",
    "# Probas test + blend + seuil optimisé\n",
    "probas_test = np.column_stack([fitted[n].predict_proba(X_testdf)[:,1] for n in names])\n",
    "blend_test  = (probas_test * best['w']).sum(axis=1)\n",
    "preds_test  = (blend_test >= best['th']).astype(int)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': pid_test,\n",
    "    'Survived': preds_test.astype(int)\n",
    "}).sort_values('PassengerId')\n",
    "\n",
    "# Sanity checks\n",
    "assert submission.shape[0]==418\n",
    "assert list(submission.columns)==['PassengerId','Survived']\n",
    "assert submission['Survived'].isin([0,1]).all()\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"✅ Wrote submission.csv — shape\", submission.shape,\n",
    "      \"| th:\", round(best['th'],3),\n",
    "      \"| weights:\", {n:round(w,3) for n,w in zip(names, best['w'])})\n",
    "\n",
    "submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13.7",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
